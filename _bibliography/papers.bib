---
---

@article{Agarwal_Rambha_Lal_Kumar_2023,
  abbr={DTA 2023},
  title={A Scalable Framework for Public Transit Assignment},
  journal={Dynamic Traffic Assignment 2023 Beyond DTA Abstracts},
  author={Agarwal, Prateek and Rambha, Tarun and Kumar, Ayush and Lal, Dhanus M},
  abstract={Public transit systems form a vital component of urban mobility. They offer a sustainable, equitable, and economical solution to transporting people. For transit to be an attractive option for travel and commuting, operators constantly aim to improve service quality. However, the benefits of such supply-side improvements heavily depend on inherently complex user behavior. Unlike personal mobility users, transit travelers typically use several criteria to choose their routes/journeys. Example criteria include in-vehicle travel time, waiting duration, walking time, cost, number of transfers, type of service (metro vs. buses), and crowding/comfort. Operators, on the other hand, have to decide routes, schedules, and fares (or modifications to existing ones) to improve key performance indicators while accounting for the changes in passenger choices that their actions may trigger.},
  year={2023},
  month={Jul},
  pages={67â€“70},
  pdf={dta2023_paper_2045.pdf},
  selected={true}

}


@misc{priyanshu2023chatbots,
      abbr={arXiv Preprint},
      title={Are Chatbots Ready for Privacy-Sensitive Applications? An Investigation into Input Regurgitation and Prompt-Induced Sanitization}, 
      author={Aman Priyanshu and Supriti Vijay and Ayush Kumar and Rakshit Naidu and Fatemehsadat Mireshghallah},
      year={2023},
      month={May},
      abstract={LLM-powered chatbots are becoming widely adopted in applications such as healthcare, personal assistants, industry hiring decisions, etc. In many of these cases, chatbots are fed sensitive, personal information in their prompts, as samples for in-context learning, retrieved records from a database, or as part of the conversation. The information provided in the prompt could directly appear in the output, which might have privacy ramifications if there is sensitive information there. As such, in this paper, we aim to understand the input copying and regurgitation capabilities of these models during inference and how they can be directly instructed to limit this copying by complying with regulations such as HIPAA and GDPR, based on their internal knowledge of them. More specifically, we find that when ChatGPT is prompted to summarize cover letters of a 100 candidates, it would retain personally identifiable information (PII) verbatim in 57.4% of cases, and we find this retention to be non-uniform between different subgroups of people, based on attributes such as gender identity. We then probe ChatGPT's perception of privacy-related policies and privatization mechanisms by directly instructing it to provide compliant outputs and observe a significant omission of PII from output. },
      eprint={2305.15008},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      pdf={2305.15008.pdf},
      selected={true}
}
